#==============================================================================================================================
#IMPORT LIBRARY
#=============================================================================================================================
import pandas as pd
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt

from wordcloud import WordCloud, STOPWORDS

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer

from sklearn.metrics import classification_report,confusion_matrix
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.metrics import roc_curve

from sklearn.naive_bayes import BernoulliNB

import streamlit as st

import pickle

from bs4 import BeautifulSoup
import re
from urllib.request import Request, urlopen

#======================================================================================================================
# 1. CODE
#======================================================================================================================
# 1.1. Read data
data = pd.read_csv("df_pre.csv", encoding='utf-8')
data = data.dropna()

def data_understand(df):
    uploaded_file = st.file_uploader("Choose a file", type = ['csv'])
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file, encoding='utf-8')
        df.to_csv("spam_new.csv", index=False)
    
    st.write("#### 1. Some data")
    st.image("image/foody_review.png")
    st.dataframe(df[['restaurant','review_text','review_score']].head())
    #st.dataframe(data[['restaurant','review_text','review_score']].tail())
    st.write('''###### D·ªØ li·ªáu review_text ch·ª©a b√¨nh lu·∫≠n c·ªßa kh√°ch h√†ng s·∫Ω l√† input.
    * review_text c√≥ icon emoji, c√≥ c√°c t·ª´ vi·∫øt t·∫Øt, teencode, vi·∫øt sai ch√≠nh t·∫£
    * C√°c t·ª´ l·∫∑p ƒëi l·∫∑p l·∫°i nhi·ªÅu nh∆∞: m√≥n ƒÉn, b√°nh, ƒë·ªì u·ªëng,...
    * T·ª´ mang nghƒ©a ph·ªß ƒë·ªãnh nh∆∞ "kh√¥ng ngon", "kh√¥ng n√™n ƒë·∫øn"... 
    * => C·∫ßn x·ª≠ l√Ω c√°c data review_text cho ng·∫Øn g·ªçn ƒë·ªÉ ƒë∆∞a v√†o model.\n\n''')
    st.markdown("###### review_text ban ƒë·∫ßu:")
    view4 = df.loc[4,'review_text']
    st.code(view4)
    st.markdown("###### review_text sau NLP:")
    view42 = df.loc[4,'review_text_clean']
    st.code(view42)

    st.write("#### 2. Visualize")
    fig1 =  plt.figure(figsize=(12,6))
    plt.subplot(1,2,1)
    sb.distplot(df.review_score)
    plt.subplot(1,2,2)
    plt.boxplot(df.review_score)
    plt.suptitle('Distplot & Boxplot of review_score')
    st.pyplot(fig1)
    
    st.write("###### D·ªØ li·ªáu t·∫≠p trung ·ªü ƒëi·ªÉm s·ªë t·ª´ 6 ƒë·∫øn 10\n\n")

    bins = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    df_view = df
    df_view['review_score_bins'] = pd.cut(df_view['review_score'], bins=bins)
    scr_bin = df_view.groupby('review_score_bins').size().sort_values(ascending=False)
    fig2 =  plt.figure(figsize=(12,6))
    scr_bin.plot.bar(color='c')
    st.pyplot(fig2)

    st.write("###### ƒê√°nh gi√° thang 7-10 chi·∫øm s·ªë l∆∞·ª£ng nhi·ªÅu, t·ªïng s·ªë t·∫ßn su·∫•t c·ªßa thang ƒëi·ªÉm <7 c·ªông l·∫°i v·∫´n √≠t h∆°n thang >7. Tuy nhi√™n n·∫øu l·∫•y m·ªëc 7 ƒë·ªÉ chia d·ªØ li·ªáu th√¨ c√°c b√¨nh lu·∫≠n ch√™ s·∫Ω kh√¥ng ƒë∆∞·ª£c th·ªÉ hi·ªán r√µ v√† d·ªØ li·ªáu v·∫´n m·∫•t c√¢n b·∫±ng => s·∫Ω d√πng c√°ch kh√°c ƒë·ªÉ c√¢n b·∫±ng d·ªØ li·ªáu v√† l·∫•y m·ªëc 6 ƒë·ªÉ ph√¢n class\n")
    st.write('''* Chia d·ªØ li·ªáu th√†nh 2 nh√≥m:
    * Nh√≥m 0: Ch√™ (Notlike): Thang ƒëi·ªÉm <=6
    * Nh√≥m 1: Khen (Like): Thang ƒëi·ªÉm > 6
    \n\n''')

    count = df['review_class'].value_counts()
    fig3 =  plt.figure(figsize=(6,6))
    count.plot.bar()
    plt.xticks(rotation=0)
    st.pyplot(fig3)

    st.write('###### D·ªØ li·ªáu kh√¥ng c√¢n b·∫±ng')

    for label, cmap in zip(['Like','Notlike'],['Reds','Greens']):
        text = str(df['review_text_clean'][df['review_class']== label].values)
        fig4 = plt.figure(figsize=(10, 6))
        wc = WordCloud(width=1000, height=600, background_color="#919191", colormap=cmap)
        wc.generate_from_text(text)
        plt.imshow(wc)
        plt.axis("off")
        plt.title(f"Words Commonly Used in ${label}$ Reviews", size=20)
        st.pyplot(fig4)
        

    #------Import html file but it is too big to show, page keep loading---------
    #HtmlFile = open("Foody_Sentiment_raw.html", 'r', encoding='utf-8')
    #source_code = HtmlFile.read()
    #components.html(source_code)

    #HtmlFile = open("Foody_Sentiment_clean.html", 'r', encoding='utf-8')
    #source_code = HtmlFile.read()
    #components.html(source_code)
    #-----------show screenshot instead--------------

    st.image("image/scatter_raw.png")
    st.write("###### V·ªõi text ch∆∞a qua x·ª≠ l√Ω, s·ª≠ d·ª•ng c√¥ng c·ª• scattertext ta th·∫•y Like (m√†u ƒë·ªè) c√≥ nhi·ªÅu icon h∆°n, Notlike c√≥ icon gi·∫≠n d·ªØ, c√°c t·ª´ xu·∫•t hi·ªán nhi·ªÅu ·ªü nh√≥m Notlike l√†: kh√¥ng bao gi·ªù, t·ªá, th√°i ƒë·ªô, th·∫•t v·ªçng...\n\n")
    st.image("image/scatter_clean.png")
    st.write('''
    * V·ªõi clean text, Notlike c√≥ c√°c t·ª´: kh√¥ng th√®m, ch·ª≠i, d·ªü, b·ª±c m√¨nh, ph√≠ ti·ªÅn, ph·ª•c v·ª• k√©m, gi·∫≠n, th·∫•t v·ªçng, kh√¥ng bao gi·ªù,  ...
    * Like c√≥ c√°c t·ª´: ·ªßng h·ªô d√†i d√†i, th√¢n thi·ªán nhi·ªát t√¨nh, trang tr√≠ ƒë·∫πp m·∫Øt, kh√¥ng gian tho√°ng m√°t, gi√° c·∫£ b√¨nh d√¢n, tuy·ªát v·ªùi, v·ª´a mi·ªáng, h·ª£p l√Ω''')


text_data = np.array(data['review_text_clean'])
# 1.2. CountVectorizer
count = CountVectorizer(max_features=1000)
count.fit(text_data)
bag_of_words = count.transform(text_data)
# 1.3. Data pre-processing
X = bag_of_words.toarray()
y = np.array(data['label'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
# 1.4. Build model
clf = BernoulliNB()
model = clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
# 1.5. Evaluate model
score_train = model.score(X_train,y_train)
score_test = model.score(X_test,y_test)
acc = accuracy_score(y_test,y_pred)
cm = confusion_matrix(y_test, y_pred, labels=[0, 1])
cr = classification_report(y_test, y_pred)
y_prob = model.predict_proba(X_test)
roc = roc_auc_score(y_test, y_prob[:, 1])
# 1.6. Save models
# luu model classication Like/Notlike
pkl_filename = "sentiment_analyse.pkl"
with open(pkl_filename, 'wb') as file:
    pickle.dump(model, file)

# luu model CountVectorizer (count)
pkl_count = "count_model.pkl"  
with open(pkl_count, 'wb') as file:
    pickle.dump(count, file)

# 1.7. Load models 
# ƒê·ªçc model
# import pickle
with open(pkl_filename, 'rb') as file:  
    sentiment_model = pickle.load(file)
# doc model count len
with open(pkl_count, 'rb') as file:  
    count_model = pickle.load(file)
    
def build_model():
    st.write("#### 1. Build model: Using BernouliNB")
    
    st.write("#### 2. Evaluation")
    st.code("Score train:"+ str(round(score_train,2)) + " vs Score test:" + str(round(score_test,2)))
    st.code("Accuracy:"+str(round(acc,2)))
    st.write("###### Confusion matrix:")
    st.code(cm)
    # visualize confusion matrix with seaborn heatmap
    cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], 
                                    index=['Predict Positive:1', 'Predict Negative:0'])
    fig1 = plt.figure(figsize=(5,5))
    sb.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')
    st.pyplot(fig1)

    st.write("###### Classification report:")
    st.code(cr)
    st.code("Roc AUC score:" + str(round(roc,2)))
    # calculate roc curve
    st.write("###### ROC curve")
    fpr, tpr, thresholds = roc_curve(y_test, y_prob[:, 1])
    fig2, ax = plt.subplots()       
    ax.plot([0, 1], [0, 1], linestyle='--')
    ax.plot(fpr, tpr, marker='.')
    st.pyplot(fig2)

    st.write("##### 3. K·∫øt lu·∫≠n")
    st.write('''
    * BernoulliNB cho k·∫øt qu·∫£ cao ·ªü c·∫£ Precision, Recall c·ªßa class 0, Accuracy 86%, FP √≠t, f1-score 0.78, th·ªùi gian th·ª±c hi·ªán: 2 gi√¢y.
    * Ngo√†i ra BernoulliNB cho k·∫øt qu·∫£ t·ªët tr√™n data g·ªëc (kh√¥ng c·∫ßn oversampling hay undersampling, undersampling s·∫Ω l√†m gi·∫£m l∆∞·ª£ng data => l√£ng ph√≠ d·ªØ li·ªáu thu th·∫≠p ƒë∆∞·ª£c, c√≤n oversampling th√¨ t·∫°o ra data gi·∫£ c≈©ng kh√¥ng ph·∫£i data th·ª±c t·∫ø thu th·∫≠p ƒë∆∞·ª£c)
    ''')


def crawl_data(link):
    req = Request(link)
    webpage = urlopen(req).read()
    soup = BeautifulSoup(webpage, "html.parser")
    name = soup.find("h1").get_text().strip()
    avg_rate = re.findall(r'\D*(\d+\.?\d*)\D*',soup.find('div', {"class":"microsite-top-points-block"}).text)[0]
    comment = soup.find_all('div', attrs={"class":"rd-des toggle-height"})
    comment_lst = [x.get_text().replace("\n...Xem th√™m","") for x in comment]
    return name, avg_rate, comment_lst

def classify_review(lines):
    if len(lines)>0:
        st.code(lines)
        x_new = count_model.transform(lines)        
        y_pred_new = sentiment_model.predict(x_new)
        if y_pred_new == 1:
            st.write(str(y_pred_new)+" - Khen üòç")
        elif y_pred_new == 0:
            st.write(str(y_pred_new)+" - Ch√™ üò°")


#======================================================================================================================
# 2. GUI
#======================================================================================================================
st.title("PROJECT 2: SENTIMENT ANALYSIS - FOODY.VN")
st.sidebar.markdown("## Choose content")


menu = ['Business Understanding','Data Understanding','Build Model']
button1 = st.sidebar.button(menu[0])
button2 = st.sidebar.button(menu[1])
button3 = st.sidebar.button(menu[2])

choice = st.sidebar.selectbox('SENTIMENT ANALYSIS',['--','From Data Input','From Website'])
    
if button1:
    st.subheader(menu[0].upper())
    st.image("image/app_foody.jpg")
    st.write("""* Foody.vn l√† m·ªôt k√™nh ph·ªëi h·ª£p v·ªõi c√°c nh√† h√†ng/qu√°n ƒÉn b√°n th·ª±c ph·∫©m online.
    * ƒê·ªÉ l·ª±a ch·ªçn m·ªôt nh√† h√†ng/qu√°n ƒÉn m·ªõi kh√°ch h√†ng c√≥ xu h∆∞·ªõng xem x√©t nh·ªØng b√¨nh lu·∫≠n t·ª´ nh·ªØng ng∆∞·ªùi ƒë√£ th∆∞·ªüng th·ª©c ƒë·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh c√≥ n√™n th·ª≠ hay kh√¥ng?
    * C√°c nh√† h√†ng/qu√°n ƒÉn c·∫ßn n·ªó l·ª±c ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng c·ªßa m√≥n ƒÉn c≈©ng nh∆∞ th√°i ƒë·ªô ph·ª•c v·ª• nh·∫±m duy tr√¨ uy t√≠n c·ªßa nh√† h√†ng c≈©ng nh∆∞ t√¨m ki·∫øm th√™m kh√°ch h√†ng m·ªõi d·ª±a tr√™n b√¨nh lu·∫≠n t·ª´ kh√°ch h√†ng
    * Ch√∫ng ta c√≥ th·ªÉ l√™n ƒë√¢y ƒë·ªÉ xem c√°c ƒë√°nh gi√°, nh·∫≠n x√©t c≈©ng nh∆∞ ƒë·∫∑t mua th·ª±c ph·∫©m.
    * T·ª´ nh·ªØng ƒë√°nh gi√° c·ªßa kh√°ch h√†ng, v·∫•n ƒë·ªÅ ƒë∆∞·ª£c ƒë∆∞a ra l√† l√†m sao ƒë·ªÉ c√°c nh√† h√†ng/ qu√°n ƒÉn hi·ªÉu ƒë∆∞·ª£c kh√°ch h√†ng r√µ h∆°n, bi·∫øt h·ªç ƒë√°nh gi√° v·ªÅ m√¨nh nh∆∞ th·∫ø n√†o ƒë·ªÉ c·∫£i thi·ªán h∆°n trong d·ªãch v·ª•/s·∫£n ph·∫©m.
    \n\n""")
    st.image("image/foody_show_review.png")
    st.write(""" V√≠ d·ª• trong h√¨nh l√† nh√† h√†ng c√≥ ƒëi·ªÉm ƒë√°nh gi√° 7.4. ƒê√¢y ch∆∞a ph·∫£i l√† ƒëi·ªÉm cao nh·∫•t n√™n nh√† h√†ng v·∫´n c·∫ßn xem ƒë√°nh gi√° t·ª´ kh√°ch h√†ng ƒë·ªÉ t·ª´ ƒë√≥ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng d·ªãch v·ª•. Nh√† h√†ng c√≥ 3 b√¨nh lu·∫≠n k√©m, 3 b√¨nh lu·∫≠n trung b√¨nh. """)
    st.write("##### M·ª•c ti√™u: ƒê√°nh gi√° n·ªôi dung c√°c b√¨nh lu·∫≠n ƒë·ªÉ t√¨m ra ƒë√¢u l√† nh·ªØng b√¨nh lu·∫≠n ch√™, k√©m, ƒë√°nh gi√° th·∫•p ƒë·ªÉ t·ª´ ƒë√≥ nh√† h√†ng bi·∫øt nh·ªØng ch·ªó c·∫ßn c·∫£i thi·ªán.")

elif button2:
    st.subheader(menu[1].upper())
    data_understand(data)
elif button3:
    st.subheader(menu[2].upper())
    build_model()

else:
    if choice == '--':
        st.image("image/emoji.png")
    if choice == 'From Data Input':
       st.subheader('Select Data')
       flag = False
       new_dt = None
       type = st.radio("Upload data or Input data?", options=("Upload", "Input"))
       if type=="Upload":
           # Upload file
           uploaded_file_1 = st.file_uploader("Choose a file", type=['txt', 'csv'])
           if uploaded_file_1 is not None:
                new_dt = pd.read_csv(uploaded_file_1)
                new_dt = new_dt.dropna()
                st.dataframe(new_dt)
                if 'review_text_clean' in new_dt.columns:
                    new_dt = new_dt['review_text_clean'].head(1)
                else:
                    new_dt = new_dt.select_dtypes(include=object)
                    new_dt = new_dt.iloc[0,:]
                flag = True
       if type=="Input":        
           review = st.text_area(label="Input your review:")
           submit2 = st.button("Submit")
           if submit2 and review!="":
               new_dt = np.array([review])
               flag = True
    
       if flag:
           classify_review(new_dt)
    
    elif choice == 'From Website':
       resto = st.text_input(label="Nh·∫≠p link nh√† h√†ng:")
       if resto!= "":
            if resto.startswith("https://www.foody.vn/"):
                if resto.endswith("/binh-luan"):
                    pass
                else:
                    sufix = "/binh-luan"
                    resto =  resto+sufix
                name, rate, comments =  crawl_data(resto)
                st.write("#### Nh√† h√†ng: "+name)
                st.write("#### Rating: "+rate)
                if len(comments)>0:
                   for comm in comments[:3]:
                       new_dt = np.array([comm])
                       classify_review(new_dt)
                else:
                   st.write("Not found data")
            else:
                st.write("Only get link restaurants on Foody.vn")
    
st.sidebar.markdown("#### by Nguyen Ha Phuong Thao")